{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>I. Reading Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#read the training file such that first line is header, file is tab delimited, ignore the double quotes\n",
    "train=pd.read_csv(r\"C:\\Users\\Priyanka\\Documents\\Priya_Documents\\IU_Data_Science\\NLP_IMDB_review\\Data\\labeledTrainData.tsv\",\\\n",
    "                 header=0,delimiter=\"\\t\",quoting=3)\n",
    "\n",
    "#read the test file such that first line is header, file is tab delimited, ignore the double quotes\n",
    "test=pd.read_csv(r\"C:\\Users\\Priyanka\\Documents\\Priya_Documents\\IU_Data_Science\\NLP_IMDB_review\\Data\\testData.tsv\",\\\n",
    "                 header=0,delimiter=\"\\t\",quoting=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "['id' 'sentiment' 'review']\n",
      "\"5814_8\"\n",
      "1\n",
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "#quick look at the data\n",
    "print(train.shape)\n",
    "print(train.columns.values)\n",
    "print(train['id'][0])\n",
    "print(train['sentiment'][0])\n",
    "print(train['review'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the above text needs cleaning. There are html tags that needs to be removed etc. etc.\n",
    "<h2>II. Cleaning and Pre-processing the data</h2>\n",
    "<ol>1. Removing HTML markup using the Beautiful Soup package</ol>\n",
    "<ol>2. Dealing with punctuation, numbers etc. using Regular Expression. To start, we will only keep the letters and space, but enhance it further by keeping emoticons like :) etc.</ol>\n",
    "<ol>3. Convert to lowercase</ol>\n",
    "<ol>4. Tokenize to words using NLTK</ol>\n",
    "<ol>5. Lemmatize using NLTK</ol>\n",
    "<ol>6. Remove stop words using NLTK</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_cleanup( raw_text ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(raw_text).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    #\n",
    "    # 3. Convert to lower case\n",
    "    words = text.lower()   \n",
    "    #\n",
    "    # 4. Tokenize\n",
    "    words=word_tokenize(words)\n",
    "    #\n",
    "    # 5. Lemmatize\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    #\n",
    "    # 6. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 7. Remove stop words\n",
    "    words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 8. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priyanka\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought wa really cool eighty maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema wa originally released ha subtle message mj feeling towards press also obvious message drug bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fan would say made fan true really nice actual feature film bit finally start minute excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord want mj dead bad beyond mj overheard plan nah joe pesci character ranted wanted people know supplying drug etc dunno maybe hate mj music lot cool thing like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually director hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away doe try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed door know fact either extremely nice stupid guy one sickest liar hope latter\n"
     ]
    }
   ],
   "source": [
    "#Text the data_cleanup function with a review\n",
    "clean_review = data_cleanup( train[\"review\"][0] )\n",
    "print (clean_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training Data Claeanup </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set movie reviews...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priyanka\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = train[\"review\"].size\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "print (\"Cleaning and parsing the training set movie reviews...\\n\")\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "for i in range( 0, num_reviews ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print (\"Review %d of %d\\n\" % ( i+1, num_reviews ) )\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_reviews.append( data_cleanup( train[\"review\"][i] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test Data Claeanup </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the test data movie reviews...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priyanka\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews_test = test[\"review\"].size\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "print (\"Cleaning and parsing the test data movie reviews...\\n\")\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_test_reviews = []\n",
    "for i in range( 0, num_reviews_test ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print (\"Review %d of %d\\n\" % ( i+1, num_reviews_test ) )\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_test_reviews.append( data_cleanup( test[\"review\"][i] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>III. Creating features from bag of words (unigram) using scikit-learn countVectorizer</h2>\n",
    "<br>\n",
    "So far we have our dataset all tidied up. Next, we need some kind of numeric representation of the data. We will use the bag of words approach to create a sparse vector with count of words. In the IMDB data, we have a very large number of reviews, which will give us a large vocabulary. To limit the size of the feature vectors, we should choose some maximum vocabulary size. Below, we use the 5000 most frequent words (remembering that stop words have already been removed).\n",
    "\n",
    "We'll be using the feature_extraction module from scikit-learn to create bag-of-words features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the bag of words....\n",
      "\n",
      "(25000, 5000)\n",
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"creating the bag of words....\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "#initialize the CountVectorizer (scikit learn's bag-of-words tool)\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, \n",
    "                            stop_words=None, max_features=5000)\n",
    "#fit_transform will fit the model to learn the vocabulary with the data and then transform into feature vector...\n",
    "train_data_features=vectorizer.fit_transform(clean_train_reviews)\n",
    "#transform the test data...\n",
    "test_data_features=vectorizer.transform(clean_test_reviews)\n",
    "\n",
    "#convert to numpy arrays\n",
    "train_data_features=train_data_features.toarray()\n",
    "test_data_features=test_data_features.toarray()\n",
    "                                        \n",
    "#print the shape\n",
    "print(train_data_features.shape)\n",
    "print(train_data_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#A quick look at the features...\n",
    "vocab=vectorizer.get_feature_names()\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>III. Classification Algorithm -scikit learn</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:\n",
      " [ 0.838   0.8424  0.8652  0.8372  0.836   0.8288  0.8332  0.8548  0.8312\n",
      "  0.8492]\n",
      "0.8416 0.0109354469502\n"
     ]
    }
   ],
   "source": [
    "# k-fold cross validation...\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "# random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest=RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "scores=cross_val_score(estimator=forest,\n",
    "                 X=train_data_features,\n",
    "                 y=train[\"sentiment\"],\n",
    "                 cv=10)\n",
    "\n",
    "print('CV scores:\\n',scores)\n",
    "\n",
    "print([np.mean(scores),np.std(scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Random Forest Classifier Score with 10 fold cross validation\n",
      "[0.84160000000000001, 0.010935446950170792]\n"
     ]
    }
   ],
   "source": [
    "#The score with random forest on training / cross-validation data\n",
    "print(\"The Random Forest Classifier Score with 10 fold cross validation\")\n",
    "print([np.mean(scores),np.std(scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:\n",
      " [ 0.8624  0.866   0.8756  0.8564  0.8392  0.8524  0.864   0.8624  0.8596\n",
      "  0.8604]\n",
      "The Logistic Regression Classifier Score with 10 fold cross validation\n",
      "[0.85984000000000016, 0.0090016887304549854]\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression claasifier with 10 fold cross validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr=LogisticRegression(random_state=0)\n",
    "\n",
    "scoresLR=cross_val_score(estimator=lr,\n",
    "                         X=train_data_features,\n",
    "                         y=train[\"sentiment\"],\n",
    "                         cv=10)\n",
    "\n",
    "print('CV scores:\\n',scoresLR)\n",
    "\n",
    "#The score with Logistic Regression on training / cross-validation data\n",
    "print(\"The Logistic Regression Classifier Score with 10 fold cross validation\")\n",
    "print([np.mean(scoresLR),np.std(scoresLR)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Up until this point, we have trained the classifier on 90 percent of the data...\n",
    "#We will now need to fit the classifiers on the entire datasets...This step is important as we do not \n",
    "#want to waste any training data.\n",
    "forest.fit(train_data_features,train[\"sentiment\"].values)\n",
    "lr.fit(train_data_features,train[\"sentiment\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predict test data sentiment and prepare the output file for kaggle </h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "resultRF = forest.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "outputRF = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":resultRF} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "outputRF.to_csv( r\"C:\\Users\\Priyanka\\Documents\\Priya_Documents\\IU_Data_Science\\NLP_IMDB_review\\results\\Bag_of_Words_model_RF.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "resultLR = lr.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "outputLR = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":resultLR} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "outputLR.to_csv( r\"C:\\Users\\Priyanka\\Documents\\Priya_Documents\\IU_Data_Science\\NLP_IMDB_review\\results\\Bag_of_Words_model_LR.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
